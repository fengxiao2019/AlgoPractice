# 最简单的爬虫系统需要完成的工作
- 给定一组URLs，下载所有由URLs指向的网页。
- 从这些网页中提取URLs。
- 将新的URL添加到要下载的URL列表中。重复这3个步骤。
涉及到的算法：BFS
爬虫系统的复杂性取决于数据的规模，数据规模越大，系统也会越复杂。
# 场景
- 搜索引擎索引。这是最常见的使用情况。爬虫收集网页，为搜索引擎创建一个本地索引。例如，谷歌机器人是谷歌搜索引擎背后的网络爬虫。
- 网络归档。这是一个从网络上收集信息的过程，以保存数据供将来使用。例如，许多国家图书馆运行爬虫来存档网站。著名的例子是美国国会图书馆1和欧盟的网络档案。
- 网页监控。
- 大数据。
# 明确设计的目标
面对不同规模和不同场景下的爬虫，设计的方案也不同，所以，需要明确需求，确保双方在需求上达成一致。
## 需要明确的问题
爬虫的场景/每月爬取的网页规模/要解析的内容（限定html）/是否需要考虑新增网页或者编辑网页/考虑存储规模/重复的网页（去重）
我们在这里限定：爬虫的场景是搜索引擎索引，每月爬取1千万张网页，需要考虑新增或网页的编辑，内容需要存储，重复的网页需要去重
除此之外，此系统还需要考虑：
- 伸缩性。数十亿个网页，网络抓取应该是非常高效，使用并行化。
- 稳健性。网络中充满了陷阱，坏的HTML、无反应的服务器、崩溃、恶意链接等都很常见。爬虫必须处理所有这些边缘情况。
- 礼貌性。爬虫不应该在很短的时间间隔内向一个网站发出过多的请求。
- 可扩展性。该系统是灵活的，因此需要最小的变化来支持新的内容类型。例如，如果我们想在将来抓取图像文件，我们应该不需要重新设计整个系统。
## 容量评估
假设每个月下载10亿的网页。
```python
qps = pow(10, 9) / (30*24*3600) = 400 
```
也就是每秒处理400个网页
评估下峰值
```python
peak = qps * 3 = 1200
```

假设每个网页的大小为500k，那么每月产生的数据大小为
```python
pow(10, 9) * 500k = 500TB
```
5年产生的数据规模为30PB。
# 系统设计
![][image-1]
## 模块说明
### Seed URLs
爬虫使用种子URLs作为爬行过程的起点。例如，要抓取一所大学网站的所有网页，选择种子URL的一个直观方法是使用该大学的域名。
种子网址的选择是一个开放式的问题。
选择Seed URLs的策略：
- 基于地域性，因为不同的国家可能有不同的流行网站。
- 基于主题：例如，我们可以将URL空间划分为购物、体育、保健等。
### URL Frontier
将URL的状态分为两种：待下载和已下载。存储待下载的URL的组件被称为URL Frontier。你可以把它称为先进先出（FIFO）队列。
### Html Fetcher
从网络下载URL内容，URL由URL Frontier提供。
### DNS Resolver
将URL 解析成对应的ip地址。
### Extractor
解析网页内容，将网页内容存储在DB，同时将网页中的链接存储在URL Frontier。
## 设计相关细节
### DFS vs BFS
你可以把网络想象成一个有向图，其中网页作为节点，超链接（URL）作为边。抓取过程可以被视为从一个网页到其他网页的有向图的遍历。两种常见的图形遍历算法是DFS和BFS。然而，DFS通常不是一个好的选择，因为DFS的深度可能很深。
BFS通常被网络爬虫使用，由先进先出（FIFO）队列实现。在先进先出队列中，URL是按照它们被排队的顺序来排队的。然而，这种实现方式有两个问题。
1. 来自同一网页的大多数链接都被链接回同一主机。在下图中，wikipedia.com的所有链接都是内部链接，使得爬虫忙于处理来自同一主机（wikipedia.com）的URL。当爬虫试图平行下载网页时，维基百科服务器将被请求淹没。这被认为是 “impolite”
	![][image-2]
2.  标准的BFS没有考虑到URL的优先级。网络很大，不是每个页面都有相同的质量和重要性。因此，我们可能希望根据页面排名、网络流量、更新频率等来确定URL的优先级。
#### URL Frontier
URL Frontier是确保politness、URL优先性和新鲜度的一个重要组成部分。在参考资料中提到了一些关于URL前沿的值得注意的论文。这些论文的结论如下。
**Politness**
一般来说，爬虫应该避免在短时间内向同一主机服务器发送太多的请求。发送过多的请求被认为是 “Implitness”，甚至被视为拒绝服务（DOS）攻击。例如，在没有任何约束的情况下，爬虫每秒可以向同一个网站发送成千上万的请求。这可能使网络服务器不堪重负。
执行”Politness”的一般想法是，每次从同一主机下载一个页面。在两个下载任务之间可以增加一个延迟。礼貌性约束是通过维护网站主机名到下载（工作者）线程的映射来实现的。**每个下载者线程都有一个单独的FIFO队列**，并且只下载从该队列中获得的URLs。下图显示了管理”Politness”的设计。
![][image-3]
- Queue Router: 它确保每个队列（b1，b2，...bn）只包含来自同一主机的URL。
- Mapping Table: 它将每个主机映射到一个队列。
- FIFO队列queue1、queue2到queue：每个队列包含来自同一主机的URL。
- Queue Selector。每个worker thread都被映射到一个队列，它只从该队列下载URL。队列选择逻辑是由队列选择器完成的。
- Work Thread 1至N。一个工作线程从同一主机逐一下载网页。在两个下载任务之间可以添加一个延迟(解决‘Implitness’)。
** Priority**
一个关于苹果产品的讨论论坛上的随机帖子与苹果主页上的帖子具有非常不同的权重。尽管它们都有 "苹果 "这个关键词，但爬虫首先抓取苹果主页是明智之举。
我们根据有用性来确定URL的优先级，有用性可以通过**PageRank**、**网站流量**、**更新频率**等来衡量。"Prioritizer "是处理URL优先级的组件。请参考参考资料。
图9-7显示了管理URL优先权的设计。
![][image-4]
- Prioritizer。它将URL作为输入并计算出优先级。
- 队列f1至fn：每个队列都有一个分配的优先级。具有高优先级的队列以更高的概率被选中。
- Queue selector。随机选择一个队列，偏向于具有较高优先级的队列。
将上述两种实现进行结合
![][image-5]
**Freshness**
网页在不断被添加、删除和编辑。网络爬虫必须定期重新抓取下载的网页，以保持我们的数据集新鲜。重新抓取所有的URL是非常耗时和耗资源的。以下是一些优化 Freshness 的策略。
- 根据网页的更新历史进行重新抓取。
- 对URL进行优先排序，首先更频繁地重新抓取重要网页。
**存储URL Frontier**
在现实世界的搜索引擎的抓取中，URL数量可能是数以亿计的。把所有的东西都放在内存中既不具备持久性也不具有可扩展性。把所有东西都放在磁盘里也不可取，因为磁盘很慢；而且它很容易成为抓取的瓶颈。
我们采用了一种**混合方法**。大部分的URL都存储在磁盘上，所以存储空间不是问题。为了减少从磁盘上读取和写入磁盘的成本，我们在内存中维护缓冲区，用于enqueue/dequeue操作。缓冲区中的数据会定期写入磁盘。
### HTML Fetcher
#### Robots.txt
#### 性能优化
**并行爬**
为了实现高性能，抓取工作被分配到多个服务器，每个服务器运行多个线程。
**DNS Resolver 缓存**
DNS解析器是爬虫的一个瓶颈，因为由于许多DNS接口调用是同步的，DNS请求可能需要时间。DNS响应时间从10ms到200ms不等。一旦爬虫线程对DNS进行了请求，其他线程就会被阻断，直到第一个请求完成。维护我们的DNS缓存以避免频繁调用DNS是一种有效的速度优化技术。我们的DNS缓存保持域名到IP地址的映射，并通过**cron作业**定期更新。
![][image-6]
**地域性**
按地理分布抓取服务器。当爬行服务器离网站主机较近时，爬行者的下载时间会更快。设计地域性适用于大多数系统组件：抓取服务器、缓存、队列、存储等。
**短暂的超时**
有些网络服务器响应缓慢，或者根本不响应。为了避免漫长的等待时间，规定了一个最大的等待时间。如果一个主机在预定的时间内没有反应，爬虫就会停止工作，抓取其他一些网页。
#### 高可用
除了性能优化，高可用也是一个重要的考虑因素。我们提出一些方法来提高系统的可用性。
- 一致性hash算法：这有助于在下载者之间分配负载。一个新的下载器服务器可以使用一致性hash算法添加或删除。
-  保存抓取状态和数据。为了防止故障，爬行状态和数据被写入一个存储系统。通过加载保存的状态和数据，可以很容易地重新启动中断的爬行。
- 异常处理。在一个大规模的系统中，错误是不可避免的，也是常见的。爬虫必须优雅地处理异常，而不会使系统崩溃。
- 数据验证。这是防止系统错误的一个重要措施。
#### 可扩展性（Extensibility）
由于几乎每个系统都在不断发展，设计目标之一是使系统足够灵活，以支持新的内容类型。抓取器可以通过插入新的模块来扩展。
#### 检测并避免有问题的内容
本节讨论检测和预防**冗余**、**无意义或有害内容**的问题。
##### 冗余的内容
存在多个网址指向同一个网页内容，如前所述，近30%的网页是重复的。哈希值或校验和有助于检测重复内容，例如MD5算法。
- checksum的存储
评估checksum的容量，大概100多G，可以用一个比较大的内存，把所有的checksum都存储进去。也可以使用20%的容量去存储，利用LRU驱逐策略。
当然，也可以借助bloom filter，但是会存在误判，这可以通过调整bloom filter算法提高精度。
##### 爬虫陷阱
蜘蛛陷阱是一个导致爬虫处于无限循环的网页。例如，一个无限深的目录结构列举如下：
```python
www.spidertrapexample.com/foo/bar/foo/bar/foo/bar/...
```
这种蜘蛛陷阱可以通过为URLs设置最大长度来避免。然而，没有一个放之四海而皆准的解决方案来检测蜘蛛陷阱。含有蜘蛛陷阱的网站很容易被识别，因为在这类网站上发现的网页数量异常多。很难开发出避免蜘蛛陷阱的自动算法；然而，用户可以手动验证和识别蜘蛛陷阱，并将这些网站从爬虫程序中排除，或者应用一些定制的URL过滤器。
#####  无效数据
有些内容几乎没有价值，如广告、代码片段、垃圾邮件的URL等。这些内容对爬虫没有用处，如果可能的话，应该被排除。
## 总结
在这一章中，我们首先讨论了一个好的爬虫的特征：可伸缩性、礼貌性、可扩展性和健壮性。然后，我们提出了一个设计，并讨论了关键的组成部分。构建一个可扩展的网络爬虫并不是一件轻而易举的事情，因为网络是非常大的，而且充满了陷阱。尽管我们已经涵盖了许多主题，但仍然遗漏了许多相关的谈话内容。
- 服务器端渲染。众多的网站使用JavaScript、AJAX等脚本来即时生成链接。如果我们直接下载和解析网页，我们将无法检索动态生成的链接。为了解决这个问题，我们在解析网页之前先进行服务器端的渲染（也叫动态渲染）。
- 过滤掉不需要的页面。由于存储容量和抓取资源有限，反垃圾邮件组件有利于过滤掉低质量和垃圾邮件页面。
- 数据库复制和分片。复制和分片等技术被用来提高数据层的可用性、可扩展性和可靠性。
- 横向扩展。对于大规模的抓取，需要数百甚至数千台服务器来执行下载任务。关键是要保持服务器的无状态。
- 可用性、一致性和可靠性。这些概念是任何大型系统成功的核心。我们在第1章中详细讨论了这些概念。刷新你对这些主题的记忆。
- 分析。收集和分析数据是任何系统的重要组成部分，因为数据是微调的关键成分。

## 引用
[https://www.ics.uci.edu/\~lopes/teaching/cs221W12/slides/Lecture05.pdf][1]



[1]:	https://www.ics.uci.edu/~lopes/teaching/cs221W12/slides/Lecture05.pdf

[image-1]:	https://tva1.sinaimg.cn/large/008i3skNly1gsaxifx22yj31600u0adm.jpg
[image-2]:	https://tva1.sinaimg.cn/large/008i3skNly1gsb38p37wwj318q0t8af8.jpg
[image-3]:	https://tva1.sinaimg.cn/large/008i3skNly1gsb3qbu717j30u0117q6s.jpg
[image-4]:	https://tva1.sinaimg.cn/large/008i3skNly1gsb45z7zydj60u017pjtp02.jpg
[image-5]:	https://tva1.sinaimg.cn/large/008i3skNly1gsb4ao4i6oj31920u010m.jpg
[image-6]:	https://tva1.sinaimg.cn/large/008i3skNly1gsbpjh1za1j319o0u0agu.jpg