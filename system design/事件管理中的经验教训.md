事件管理中的经验教训
在Dropbox，我们将事件管理视为我们可靠性工作的核心要素。虽然我们也采用了混沌工程等主动技术，但我们对事件的反应对用户的体验有很大影响。在潜在的网站中断或产品问题中，我们的用户每一分钟都很重要。

我们的事件管理流程的关键部分已经存在了好几年，但我们也发现了在这个领域不断发展的机会。随着时间的推移，我们所做的调整包括技术、组织和程序方面的改进。

这篇文章深入探讨了Dropbox在事件管理方面的一些经验教训。您可能不会在教科书中找到所有这些关于事故指挥结构的描述，您也不应该将这些改进视为适合每个公司的万能方法。它们的实用性将取决于你的技术栈、组织规模和其他因素）。相反，我们希望这可以作为一个案例研究，说明你如何能够系统地看待你的组织自己的事件响应，并发展它以满足你的用户的需求。

Dropbox管理事件的基本框架，我们称之为SEV（如SEVerity），与许多其他SaaS公司采用的框架类似。对于那些不太熟悉这个主题的人，我们推荐Dropbox校友Tammy Butow的这一系列教程，以获得一个概述）。

可用性SEVs，尽管绝不是唯一的关键事件类型，但却是一个有用的切片，可以更深入地探索。没有任何在线服务可以免受这些事件的影响，这包括Dropbox。关键可用性事件往往是对最多用户造成破坏的事件--只要想想您最喜欢的网站或SaaS应用程序上次发生故障的时间，所以我们发现这些SEV对我们事件响应的及时性造成了最大的压力。成功意味着从响应中减去每一分钟。

我们不仅密切衡量可用性SEVs的影响时间，而且这一指标也有实际的商业后果。每一分钟的影响都意味着更多不满意的用户、更多的用户流失、更少的注册用户，以及社交媒体和媒体对故障的报道带来的声誉损失。除此之外，Dropbox在与我们一些客户的合同中承诺了正常运行时间SLA，特别是在关键任务行业。我们根据为用户提供服务的系统的整体可用性来定义，当我们的可用性下降到一定的阈值时，就正式从 "正常 "变为 "故障"。为了保持在99.9%的正常运行时间的SLA内，我们必须将任何停机时间限制在每月共43分钟左右。我们在内部为自己设定了更高的标准，目标是99.95%（每月21分钟）。

为了保障这些目标，我们在各种事故预防技术方面进行了投资。这些技术包括混沌工程、风险评估和验证生产要求的系统，仅举几例。然而，无论我们如何探究和努力了解我们的系统，SEVs仍然会发生。这就是事件管理的意义所在。

## SEV 流程
Dropbox 的 SEV 流程规定了我们的各种事件响应角色如何合作以减轻 SEV，以及我们应采取哪些步骤从事件中学习。

Dropbox的每个SEV都有几个基本特征。

- SEV 类型，对事件的影响进行分类: 著名的例子包括_可用性_、_耐用性_、_安全性_和_功能退化_。
- SEV级别，从0-3，表示严重程度；0是最严重的。
- 一个IMOC（待命事件经理），负责带头快速缓解，协调SEV响应者，并传达事件的状态。
- TLOC（待命技术负责人），负责推动调查并做出技术决定。
对客户有影响的SEV还包括**一个BMOC（业务经理待命）**，他管理非工程职能，在需要时提供外部更新。根据不同的情况，这可能包括状态页面更新、直接与客户沟通，以及在极少数情况下的监管通知。

Dropbox已经建立了自己的事件管理工具，我们称之为DropSEV。任何Dropbox用户都可以声明一个SEV，这将触发上述角色的分配，并创建一组通信渠道来处理该事件。这些渠道包括一个用于实时协作的**Slack频道**，一个用于更广泛更新的**电子邮件线程**，一个用于收集工件和数据点的**Jira票据**，以及一个预先填充的事后总结文件，以便日后用于回顾。员工也可以订阅一个特定的Slack频道或电子邮件列表，以看到创建的新SEV的流。这里是一个样本DropSEV条目，为我们的移动应用程序宣布一个小的可用性事件。

虽然后一阶段有很多细微的差别，即产生有价值的事后总结和有效的行动项目--参见Google SRE工作手册中的 "**事后总结文化**"--本篇文章将更多地关注SEV被缓解之前的阶段，同时用户体验仍然受到影响。为了简化，我们把它分成三个总体阶段。

- 检测。识别问题并提醒响应者所需的时间
- 诊断。响应者从根本上解决一个问题，和/或确定一个解决方法所需的时间
- 恢复。一旦我们有了解决方法，为用户缓解问题所需的时间。
还记得我们在一个月内停机时间的最大目标是21分钟吗？这对于检测、诊断和恢复一个复杂的技术问题来说并不是很多时间。我们发现，我们必须优化这三个阶段才能实现。
## 3.  检测
**识别问题并提醒响应者所需的时间**

**可靠、高效的监控系统**
在Dropbox检测问题时，我们的监控系统是一个关键组成部分。多年来，我们已经建立并完善了几个系统，工程师们在事件发生时依靠这些系统。首先是_Vortex_，**我们的服务器端指标和警报系统**。Vortex提供了**几秒钟的摄取延迟**，**10秒钟的采样率**，以及一个简单的接口，让服务部门定义他们自己的警报。

这些功能是缩短生产中发现问题的时间的关键。[在之前的一篇博文中][1]，我们介绍了技术设计，如下图所示。
![][image-1]
2018年该系统的重新设计是我们可靠性努力的基础。要了解原因，想想每月21分钟的内部停机时间目标。我们需要知道，在事件开始的几十秒内，Vortex会接收到这些信号并通过PagerDuty提醒正确的响应者。如果Vortex不可靠或速度慢，我们的响应在开始之前就会受到阻碍。

你的组织可能没有使用像我们这样的本土监控系统，但问问你自己：它的信号能多快地触发你的事件响应？

**优化指标和警报**
Vortex是快速预警问题的关键，但如果没有明确的指标来预警，它就没有用。在很多方面，这是一个很难解决的问题，因为总有一些特定于用例的指标需要团队自己添加。

我们试图通过提供一套丰富的服务、运行时和主机指标来减轻服务所有者的负担。这些指标被植入我们的RPC框架Courier和我们的主机级基础设施。除了一系列标准指标外，Courier还提供**分布式跟踪和剖析**，以进一步帮助事件的处理。Courier为我们在Dropbox使用的所有语言（Go、Python、Rust、C++、Java）提供相同的指标集。

虽然这些开箱即用的指标非常有价值，**但嘈杂的警报也是一个常见的挑战，往往让人很难知道某个页面是否是为了一个真正的问题**。我们提供了几个工具来帮助缓解这个问题。最强大的是一个警报依赖系统，服务所有者可以将他们的警报与其他警报联系起来，如果问题出现在一些共同的依赖中，就可以让页面保持沉默。这使得团队可以避免因为那些他们无法操作的问题而被呼唤，并更快地对真正的问题采取行动。

**将人的因素从提交事件中剔除**
我们的团队从PagerDuty收到各种各样的关于其系统健康状况的警报，但并非所有这些都是 "值得SEV "的。从历史上看，这意味着收到页面的Dropbox第一反应人不仅要担心修复问题，还要担心它是否值得提交SEV并启动正式的事件管理流程。

这种区别可能会让人感到困惑，特别是对于那些作为团队的待命人员经验不足的人。为了使决策更简单，我们改造了DropSEV（前面描述的事件管理工具），将SEV的定义直接呈现给用户。例如，如果你在DropSEV中选择 "可用性 "作为SEV类型，它会弹出一个像这样的表格，将全球可用性影响程度映射到SEV级别。
![][image-2]

这是一个进步，但我们意识到 "我是否要提交SEV？"的决定仍然拖累了我们的响应者。假设你是Magic Pocket的前端组件的值班人员，我们的内部多字节存储系统，负责处理数据块的获取和投放请求。当你收到一个页面时，你必须问自己一系列的问题。

可用性下降了吗？
下降了多少？
这是否对全球可用性产生了上游影响？仪表板在哪里？
影响有多大？如何与SEV表保持一致？
这不是一个顺利的程序，除非你是一个训练有素的值班人员，而且你以前见过你的SEVs的份额。即使在最好的情况下，在一次停电中，它也会使我们在21分钟的关键预算中损失几分钟的时间。有几次，SEV甚至没有提交，这意味着我们错过了IMOC和BMOC的参与，而技术团队正在努力恢复可用性。

因此，今年夏天，我们开始自动提交所有的可用性SEVs。服务所有者仍然会收到他们自己的系统警报，但DropSEV将检测SEV值的可用性影响，并自动启动正式的事件响应过程。服务所有者不再需要为提交SEV而分心，我们有更高的信心，所有的事件响应角色都会参与进来。更重要的是，对于每一个可用性SEV，我们的响应时间都缩短了几分钟。


在您自己的事件响应流程中，您可以在哪些方面缩短人为决策？

**诊断**
响应者从根本上解决一个问题，和/或确定一个解决方法所需的时间
共同待命(0n-call)标准
在诊断阶段，我们经常需要拉拢更多的响应者来帮助最初的警报接收者。我们在内部服务目录中建立了一个 "呼唤值班人员 "的按钮，因此，如果需要，人类可以有效地联系到另一个团队。这是我们多年来使用PagerDuty的另一种方式。
![][image-3]

然而，我们发现了一个关键的变量，它使我们的中断时间无法持续保持在21分钟以下。一个团队的PagerDuty设置是如何配置的？随着时间的推移，Dropbox的善意团队对类似这样的问题做出了非常不同的决定。

我们的升级政策需要多少层？
每次升级之间应该有多少时间？在整个升级链中？
我们的值班人员应该如何接收PagerDuty的通知？他们是需要推送、短信、还是电话，或者是组合？
在解决了几个SEV中的这些不一致问题后，我们建立了一套待命检查，根据共同的准则评估每个团队的PagerDuty设置。我们建立了一个内部服务，用于查询PagerDuty的API，运行我们想要的检查逻辑，并就任何违规行为与团队联系。

我们做出了一个艰难的决定，即严格执行这些检查，不允许有任何例外。这是一个具有挑战性的转变，因为团队已经习惯了一些灵活性，但对上述问题有一致的答案，使我们的事件响应有了更大的可预测性。在最初的积累之后，很容易迭代增加额外的检查，我们找到了使我们的初始集更细微的方法（例如，根据你的团队的服务的关键性，有不同的标准）。反过来，你自己的待命准则应该是你自己组织中对事件响应的业务要求的一个功能。

截至本文发表时，PagerDuty已经发布了一份待命准备报告（针对他们的一些计划），允许你在他们的平台上进行一些类似的检查。虽然覆盖面与Dropbox内部构建的内容不尽相同，但如果你想快速建立一些一致性，它可能是一个很好的起点。


分流仪表板
对于我们最关键的服务，如驱动dropbox.com的应用程序，我们已经建立了一系列的分流仪表板，收集所有的高级指标，并提供一系列的路径来缩小调查的重点。对于这些关键系统，减少分流所需的时间是首要任务。这些仪表盘减少了从一般可用性页面到找到故障系统所需的努力。

针对常见根本原因的开箱即用的仪表板
虽然没有两个事件或后端服务是相同的，但我们知道某些数据点对我们的事件响应者来说一次又一次地有价值。仅举几例。

客户端和服务器端的错误率
- RPC 延迟
- 异常趋势
- 每秒查询次数（QPS
- 异常主机（如错误率较高的主机）。
- 顶级客户
为了缩短诊断时间，我们希望每个团队在最需要的时候都能得到这些指标，这样他们就不会浪费宝贵的时间去寻找能够指出根本原因的数据。为此，我们建立了一个开箱即用的仪表盘，涵盖上述所有内容，甚至更多。对于Dropbox的新服务所有者来说，除了将页面加入书签外，建立工作是零。我们也鼓励服务所有者为他们团队的特定指标建立更细微的仪表盘）。

拥有这样一个共同平台的力量在于，你可以轻松地随着时间的推移进行迭代。我们是否在事件中看到了新的根本原因模式？很好--我们可以在通用仪表盘上添加一个面板，以浮现这些数据。我们还投资在Grafana中添加注释，将关键事件（代码推送、DRT等）叠加到指标上，以帮助工程师进行关联。这些迭代中的每一项都能将整个公司的诊断时间缩小一点。

异常跟踪
Dropbox拥有的用于诊断问题的最高信号工具之一是我们的**异常跟踪基础设施**。它允许Dropbox的任何服务将堆栈痕迹排放到中央存储，并以有用的元数据标记它们。前端允许开发人员轻松查看和探索其服务中的异常趋势。这种深入研究异常数据和分析趋势的能力在诊断我们较大的python应用程序的问题时是非常有用的。

**事件经理的角色：清除干扰因素**
在停电或其他关键事件中，当SEV团队在诊断问题时，很多利益相关者都会关注正在发生的事情。

面向客户的团队需要提供更新和解决的ETA。
在受影响系统的爆炸半径内的服务所有者对技术细节很好奇。
对可靠性和业务目标负责的高级领导，希望传达紧迫性。
特别是高级工程领导可能想卷起他们的袖子，加入诊断工作。
更糟糕的是，2020年，由于响应者开始在家工作，无法当面协作，你的事件Slack频道中的串扰可能已经增加。

在Dropbox，我们已经看到上述所有情况在我们的事件中展开。然而，我们有时会纠结于IMOC的一个关键因素：为SEV团队屏蔽这些干扰。由于接受过培训，IMOC一般都知道SEV流程、相关的术语和工具，以及对事后分析和事件审查的期望。但他们并不总是知道如何驾驶作战室和优化我们SEV响应的效率。我们听到我们的工程师反馈说，IMOC没有为他们提供所需的一线支持，而且分散的注意力拖慢了他们诊断问题的速度。

我们意识到，我们没有明确IMOC角色的这些方面，而且 "什么是好的IMOC "的部落知识已经随着时间的推移而消失了。第一步是更新我们的培训，以强调设置紧迫性，清除干扰，并在一个地方巩固沟通。我们现在正致力于开发类似游戏的SEV场景，让新的IMOC在第一次轮班前就能实际练习这些概念。最后，我们计划增加桌面演习的频率，让更多的IMOC参与进来，以便小组能够定期评估其整体准备情况。

此外，我们建立了一个由高级IMOCs和TLOCs组成的后备响应小组，在发生最严重的事件时，可以将他们拉进来。我们给他们一个明确的游戏规则，以评估事件的状况，并与现有的IMOC/TLOC一起决定是否应该转移所有权。在必要的时候，给这些高级成员一个明确的、众所周知的角色，使他们成为一个有价值的支持结构，而不是Slack中的一组额外的声音。

关键的教训是：注意你的事件响应流程在纸面上的样子和它在实践中的运作方式之间的差距。

恢复
一旦我们有了解决方法，为用户缓解问题所需的时间

设想缓解方案
起初，要为恢复阶段找到一个银弹似乎很困难。当每个事件都需要不同的缓解策略时，你如何优化这个部分？这需要深入了解您的系统如何运作，以及您的事件响应者在最坏情况（但可行）下必须采取的步骤。

随着Dropbox追求99.9%的正常运行时间SLA，我们开始每季度进行一次可靠性风险评估。这是一个自下而上的集思广益的过程，在我们的基础设施团队和其他系统可能涉及SEV的团队中进行。由于知道我们需要优化恢复时间，我们促使各团队关注一个简单的问题。"你的系统的哪些事故场景需要20分钟以上的时间来缓解？" 

这就浮现出各种理论上的事件，其可能性程度各不相同。如果这些事件中的任何一个发生，我们都不可能保持在我们的停机时间目标之内。当我们把风险评估结果提交给基础设施领导层时，我们一致认为哪些是值得投资的，几个团队开始着手消除这些情况。举几个例子来说。

我们的单体后端服务的推送时间超过20分钟。该服务的所有者将部署管道优化到20分钟以下，并开始定期运行DRT，以确保推送时间不会降低。
如果我们在单机架故障中失去了足够多的主副本，那么推广备用数据库副本可能需要20分钟以上。我们的元数据团队改善了我们数据库系统的机架多样性，并加强了处理数据库推广的工具。
实验和功能门的变化难以识别，核心团队在紧急情况下无法回滚这些变化，这使得解决一个与实验有关的问题可能需要20分钟以上。因此，我们的实验团队提高了变化的可见性，确保所有的实验和功能门都有一个明确的所有者，并为我们的中央值班人员提供了回滚能力和一个游戏手册。
通过解决这些和其他情况，我们看到冗长的可用性事件的数量急剧下降。我们继续使用 "20分钟规则 "作为衡量我们发现的新情况的标准，可以说，随着时间的推移，应该收紧这个门槛。

你可以在你自己的组织或团队中采用类似的方法。**写下处理时间最长的潜在事件**，**按可能性进行堆叠排序，并改进列表中最重要的事件**。然后，用DRT或 "不幸之轮 "演习来测试该场景在你的事件响应者中的实际表现。这些改进是否将恢复时间减少到你的企业可以接受的水平？

保持在99.9的水平
在严格的服务水平协议下，当事情出错时，我们没有太多的时间做出反应，但我们的技术栈在我们脚下不断变化。下一个风险会在哪里出现，我们的投资重点又在哪里？

除了上面提到的流程变化，为了帮助守住底线，我们已经在两个领域建立了权威的数据源。

- 一个关于我们的服务水平协议的单一真相来源，以及哪些事件影响了它
- 一个跟踪关键路径中服务的相对影响的仪表板
我们的SLA有一个单一的真相来源，这使得我们可以直接进行规划。这确保了整个组织不会对每个团队的贡献（如内部SLA）与客户保证之间的关系产生混淆。

我们还使用**分布式跟踪来跟踪关键路径中的服务的影响**。追踪基础设施被用来计算每个服务的权重。该权重是对某项服务对Dropbox整体的重要性的一种近似估计。当一个服务的权重超过一个阈值时，就会执行额外的要求，以确保其操作的严格性。

这些权重（以及围绕它们的自动化）有两个目的。首先是作为我们风险评估过程中的另一个数据点；了解服务的相对重要性让我们更好地了解不同系统中的风险如何比较。第二个目的是确保当一个服务被添加到关键路径时，我们不会被吓到。对于像Dropbox这样规模的系统，我们很难跟踪每一个新的服务，所以跟踪关键路径可以确保我们抓住一切。

估算用户影响
"这个SEV对我们的客户有多大的痛苦？" 解决这个问题不会加速恢复，但它将使我们能够通过BMOC和值得信任（这是我们的第一个公司价值观）主动与用户沟通。同样，在事件中，每一分钟对我们的用户来说都很重要。

事实证明，这个问题对Dropbox的可用性SEV来说特别棘手。你可能已经注意到了，我们的可用性SEV级别的定义是从一个有点天真的假设开始的，即较低的可用性是更严重的。随着公司规模的扩大，这种定义提供了一些简单性，但从长远来看，它让我们失望了。我们曾经遇到过可用性受到严重打击但几乎没有对客户造成影响的SEVs；我们也遇到过勉强算作SEVs的轻微可用性打击，但使dropbox.com完全失去作用。后一种情况让我们彻夜难眠，因为我们面临着服务不足和与用户沟通不足的风险。

作为一个战术性的解决方案，我们将目标锁定在我们的网站上，因为网站的流量低于我们的桌面和移动应用程序（这意味着针对网站的可用性问题在数字上可能不那么突出）。我们与整个工程团队合作，确定了大约20条与关键网页和组件相对应的网络路线。我们开始监控这些单独路线的可用性，将这些指标添加到我们的仪表板、警报和SEV标准中。我们对IMOC和BMOC进行了培训，让他们了解如何解释这些数据，如何将可用性差异映射到具体的影响上，并与客户沟通，我们还进行了一次桌面演练。因此，在随后发生的影响我们网站的事件中，我们迅速了解到关键用户的工作流程是否受到影响，并利用这些信息与客户进行沟通。

我们相信我们在这个领域还有工作要做。我们正在探索其他各种方式，从测量9s转向测量客户体验，我们对这些将带来的技术挑战感到兴奋。

我们能不能将我们所有的路线，跨平台，分类为基于关键性的桶，并将其标记给响应团队？
我们如何利用客户影响的直接信号（如帮助和状态页面的流量、客户票据流入和社交媒体反应）来快速触发工程响应？
在我们的规模下，我们如何有效地估计受可用性事件影响的实时独特用户的数量？在事件发生后，我们如何获得有关受影响人群--区域、SKU等--的更精确的数据？
我们将在哪些方面从客户端仪器中获益最多，以衡量客户端到端的体验？

持续改进
Dropbox 在事件管理方面并不完美，没有人能够做到。我们开始时的 SEV 流程对 Dropbox 的现状非常有效，但随着我们的组织、系统和用户群的增长，我们必须不断地发展。我们在这篇文章中概述的经验并不是免费的；我们往往需要经过一两次糟糕的 SEV 才能意识到我们的事件响应的不足之处。

**这就是为什么在每次事件审查中从你的差距中学习是如此关键。你不可能完全防止关键事件的发生，但你可以防止同样的诱因再次出现在你身上**。

在Dropbox，这要从我们事件审查中的**无责文化**开始。如果响应者在事件中犯了错误，我们不会责怪他们。我们会问，**我们的工具在防止人为错误方面缺少什么护栏，我们没有给响应者提供什么培训，让他们为这种情况做好准备，以及自动化是否可以在一开始就把响应者排除在外**。这种文化使所有各方都能自如地面对SEV给我们带来的艰难教训。

正如我们不把SEV的责任归咎于个人一样，我们对Dropbox的事件管理所做的改进也不能归功于任何一个人。在过去的几年里，我们确实在全组织范围内做出了努力，以纳入这些经验教训。但仅举几个例子，我们要感谢可靠性框架、遥测、元数据、应用服务和实验团队的现任和前任成员，感谢本博文中所涉及的主题。

如果您有兴趣帮助我们将Dropbox的事件管理提升到新的水平，我们正在招聘网站可靠性工程师。随着Dropbox宣布我们正在成为一个虚拟第一的公司，这些职位的地点是长期灵活的。

[1]:	https://dropbox.tech/infrastructure/monitoring-server-applications-with-vortex

[image-1]:	https://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2021/01/incident-management/diagrams/vortex-diagram.png
[image-2]:	https://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2021/01/incident-management/diagrams/Techblog-IncidentManagement-720x285px-3.png
[image-3]:	https://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2021/01/incident-management/diagrams/Techblog-IncidentManagement-720x460px-4.png