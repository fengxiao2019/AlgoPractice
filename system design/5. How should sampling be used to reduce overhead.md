# 5. How should sampling be used to reduce overhead?
> Sampling determines which trace-point records are persisted by the tracing infrastructure. It is the most important technique used by end-to-end tracing infrastructures to limit runtime and storage overhead [9, 19, 40, 43, 47](). For example, even though Dapper writes trace-point records to stable storage asynchronously (i.e., off the critical path of the distributed system), it still imposes a 1.5% throughput and 16% response time overhead when persisting all trace points executed by a web search workload [43](). When using sampling to capture just 0.01% of all trace points, the slowdown in response times is reduced to 0.20% and in throughput to 0.06% [43](). Even when trace-point records need not be persisted because the required analyses can be performed online, sampling is useful to limit the sizes of analysis-specific data structures [9]().
采样决定了哪些追踪点的记录被追踪基础设施持续保存。它是端到端追踪基础设施用来**限制运行时间和存储开销**的最重要技术[9, 19, 40, 43, 47]()。例如，即使Dapper以异步方式（即脱离分布式系统的关键路径）将追踪点记录写入稳定的存储，但在持久化由网络搜索工作负载执行的所有追踪点时，它仍然会带来1.5%的吞吐量和16%的响应时间开销[43]()。当使用抽样来捕获所有跟踪点的0.01%时，响应时间的减慢减少到0.20%，吞吐量减少到0.06%[43]()。即使因为所需的分析可以在线进行，所以跟踪点记录不需要持久化，抽样也有助于限制特定分析数据结构的大小[9]()。
> There are three fundamentally different options for deciding what trace points to sample: head-based coherent sampling, tail-based coherent sampling, or unitary sampling. Coherent sampling methods, which guarantee that all or none of the trace points executed by a workflow will be sampled, must be used if traces showing causally-related activity are to be constructed. Additionally, head-based sampling will often result in high overheads if it is used to preserve submitter causality. Figure 4 illustrates the tradeoffs between the different sampling schemes when used to preserve different causality slices. The rest of this section further describes the sampling schemes.
在决定对哪些跟踪点进行抽样时，有三种不同的选择：基于头部的抽样，基于尾部的抽样，或者单元抽样。如果要构建显示因果关系活动的轨迹，就必须使用相关采样方法，该方法保证工作流执行的所有轨迹点都会被采样，或者不被采样。此外，如果使用基于**头部的采样**来保持提交者的因果关系，往往会导致**高额的开销**。图4 说明了不同的采样方案在用于保存不同的因果关系片断时的权衡。本节的其余部分将进一步描述这些采样方案。
Head-based coherent sampling: With this method, a random sampling decision is made for entire workflows at their start (e.g., when requests enter the system) and metadata is propagated along with workflows indicating whether to collect their trace points. The percentage of workflows randomly sampled is controlled by setting the workflow-sampling percentage. When used in conjunction with tracing infrastructures that preserve trigger causality, the workflow-sampling percentage and the trace-point-sampling percentage (i.e., the percentage of trace points executed that are sampled) will be the same. Due to its simplicity, head-based coherent sampling is used by many existing tracing implementations [19, 40, 43]().
**基于头部的连贯性采样**： 通过这种方法，在整个工作流开始时（例如，当请求进入系统时）对其进行**随机抽样**决定，**元数据与工作流一起传播**，表明是否要收集其跟踪点。随机取样的工作流的百分比是通过设置工作流取样百分比来控制的。当与保留**触发因果关系**的追踪基础设施一起使用时，工作流取样百分比和追踪点取样百分比（即被取样的追踪点的百分比）将是一样的。由于其简单性，基于头部的一致性采样被许多现有的跟踪实现所使用[19, 40, 43]()。
> Head-based coherent sampling will not reduce runtime and storage overhead for tracing infrastructures that preserve submitter causality. This is because the effective trace-point-sampling percentage will almost always be much higher than the workflow-sampling percentage. To understand why, recall that preserving submitter causality means that latent work is attributed to the original submitter. So, when latent work is aggregated by another request or background activity, trace points executed by the aggregator must be sampled if any one of the aggregated set was inserted into the system by a sampled workflow. In many systems, this process will result in sampling almost all trace points deep in the system. For example, if head-based sampling is used to sample trace points for only 0.1% of workflows, the probability of sampling an individual trace point will also be 0.1% before any aggregations. However, after aggregating 32 items, this probability will increase to 3.2% and after two such levels of aggregation, the trace-point-sampling percentage will increase to 65%. The leftmost diagram in Figure 4 illustrates this inflationary process for one level of aggregation. The overall effective trace-point-sampling percentage depends on several parameters, including the workflow-sampling percentage, the number of aggregation levels, and the number of trace points between aggregation levels. When developing the revised version of Stardust [40](), we learned of this incompatibility between head- based coherent sampling and submitter causality the hard way. Head-based sampling was the first feature we added to the original Stardust [47](), which previously did not use sampling and preserved submitter causality. But, at the time, we didn’t know anything about causality slices or how they interact with different sampling techniques. So, when we applied the sampling-enabled Stardust to our test distributed system, Ursa Minor [1](), we were very confused as to why the tracing overheads did not decrease. Of course, the root cause was that Ursa Minor contained a cache very near the entry point to the system, which aggregated 32 items at a time. We were using a sampling rate of 10%, meaning that 97% all trace points executed after this aggregation were always sampled.
**基于头部的采样**不会减少运行时间和保存**提交者因果关系**的追踪基础设施的存储开销。这是因为有效的追踪点抽样比例几乎总是比工作流抽样比例高得多。为了理解这个原因，回顾一下，保留提交者的因果关系意味着潜在的工作被归于原始提交者。因此，当潜在工作被另一个请求或背景活动聚合时，如果被聚合的集合中的任何一个是由被抽样的工作流插入系统的，那么由聚合者执行的跟踪点必须被抽样。在许多系统中，这个过程将导致对系统深处的几乎所有跟踪点进行采样。例如，如果**基于头部的抽样**只对0.1%的工作流程的跟踪点进行抽样，那么在任何聚合之前，对单个跟踪点的抽样概率也将是0.1%。然而，在聚合了32个项目之后，这个概率将增加到3.2%，经过两级这样的聚合，跟踪点采样的比例将增加到65%。图4中最左边的图说明了这种一级聚合的膨胀过程。整体有效的跟踪点采样率取决于几个参数，包括工作流程的采样率、聚合级别的数量以及聚合级别之间的跟踪点数量。

￼![][image-1]
> Figure 4: Trace points that must be sampled as a result of using different sampling schemes and preserving different causality slices. In this example, the right-most workflow in each of the four diagrams causes an on-demand eviction and, as part of this process, aggregates latent work stored in other cache blocks. Head-based sampling greedily decides whether to sample workflows at their start. As such, when preserving submitter causality using head-based sampling, all trace points executed by workflows that aggregate latent work (e.g., the workflow forcing the on-demand eviction in this example) must be sampled if any one of the aggregated set was inserted into the system by a sampled workflow. With each aggregation, the probability of sampling individual trace points will increase, resulting in high storage and runtime overheads. Since tail-based sampling defers sampling decisions to when workflows finish, it does not inflate the trace-point-sampling probability as a result of aggregations. However, it requires that records of trace points executed by workflows that leave latent work be cached until the latent work is aggregated. Unitary sampling also does not suffer from sampling inflation because it does not attempt to coherently capture workflows’ trace-point records. However, with unitary sampling, data required to obtain traces or for needed analysis must be propagated with metadata and stored in trace-point records. The rightmost diagram shows that head-based sampling can be used to preserve trigger causality with low overhead because latent work is always attributed to the aggregator. As such, only the sampling decision made for the aggregator matters when deciding whether to sample the trace points it executes.
图4：由于使用不同的抽样方案和保留不同的因果关系片，必须对跟踪点进行采样。在这个例子中，四张图中最右边的工作流引起了按需驱逐，作为这个过程的一部分，聚集了存储在其他缓存块中的潜在工作。基于头部的采样贪婪地决定是否在工作流开始时进行采样。因此，当使用基于头部的抽样来保存提交者的因果关系时，如果聚合潜在工作的工作流（例如，在这个例子中强迫按需驱逐的工作流）执行的所有跟踪点是由被抽样的工作流插入系统的，则必须被抽样。随着每次聚合，对单个跟踪点进行采样的概率将增加，从而导致高存储和运行时间开销。由于基于尾部的采样将采样决定推迟到工作流结束时，它不会因为聚集而导致跟踪点采样概率的膨胀。然而，它要求由工作流执行的跟踪点的记录留下潜在的工作被缓存，直到潜在的工作被聚集。单一抽样也不会受到抽样膨胀的影响，因为它并不试图连贯地捕捉工作流的跟踪点记录。然而，在单一抽样的情况下，获得痕迹或需要分析的数据必须与元数据一起传播，并存储在痕迹点记录中。最右边的图显示，基于头部的采样可以用来保存触发因果关系，而且开销很低，因为潜在的工作总是归于聚合器。因此，在决定是否对其执行的跟踪点进行采样时，只有为聚合器做出的采样决定是重要的。

> Tail-based coherent sampling: This method is similar to the previous one, except that the workflow- sampling decision is made at the end of workflows, instead of at their start. Delaying the sampling decision allows for more intelligent sampling—for example, the tracing infrastructure can examine a workflow’s properties (e.g., response time) and choose only to collect anomalous ones. But, trace-point records for every workflow must be cached somewhere until the sampling decision is made for them. Because many workflows can execute concurrently, because each request can execute many trace points, and because workflows with latent work will remain in the system for long periods of time, such temporary collection is not always feasible.
**基于尾部的连贯性采样** : 这种方法与前一种方法类似，只是工作流的采样决定是在工作流结束时做出的，而不是在工作流开始时。推迟取样决定可以实现更智能的取样--例如，追踪基础设施可以检查工作流的属性（例如，响应时间），并选择只收集异常的工作流。但是，每个工作流程的跟踪点记录必须被缓存在某个地方，直到对它们做出采样决定。因为许多工作流程可以同时执行，因为每个请求可以执行许多跟踪点，因为有潜伏工作的工作流程会在系统中保留很长一段时间，所以这种临时收集并不总是可行的。
> Tail-based sampling avoids inflating the trace-point-sampling percentage because it does not commit to a sampling decision upfront. As such, it can be used to preserve submitter causality with low runtime and storage overhead. For workflows that carry aggregated work, tail-based sampling guarantees that either all or none of the trace points executed by workflows whose work has been aggregated are sampled. Accomplishing this requires maintaining a mapping between aggregators’ workflow IDs and the IDs of the workflows whose work they have aggregated. The second-leftmost diagram in Figure 4 illustrates the trace-point records that must be cached when preserving submitter causality with tail-based sampling. Due to its high memory demand, tail-based sampling is not used by most tracing infrastructures.
基于尾部的抽样避免了膨胀的跟踪点抽样比例，因为它没有预先承诺一个抽样决定。因此，它可以被用来保存提交者的因果关系，而且运行时间和存储开销都很低。对于承载聚合工作的工作流，基于尾部的采样保证了所有或没有工作流执行的跟踪点被采样，而这些工作流的工作已经被聚合了。要做到这一点，需要在聚合者的工作流ID和他们所聚合的工作流的ID之间保持一个映射。图4中最左边的图说明了在用基于尾部的抽样来保护提交者的因果关系时必须缓存的跟踪点记录。由于其**高内存需求（缺点）**，基于尾部的采样不被大多数追踪基础架构所使用。
> Some tracing infrastructures use a hybrid scheme, in which they nominally use head-based coherent sampling, but also cache records of recently executed trace points in per-node circular buffers. The circular buffers are often sized to guarantee a request’s trace-point records will not be evicted as long as it’s execution time does not exceed the 50th or 75th percentile. This technique allows tracing infrastructures to backtrack and collect traces for non-sampled workflows that appear immediately anomalous (e.g., fail or return an error code soon after starting execution). However, it is not sufficient for performance anomalies (e.g., requests that take a very long time to execute).
一些追踪基础设施使用混合方案，名义上使用基于头部的一致性采样，但也在每个节点的循环缓冲区中缓存最近执行的追踪点的记录。循环缓冲区的大小通常是为了保证一个请求的跟踪点记录不会被驱逐，只要它的执行时间不超过第50或75个百分点。这种技术允许追踪基础设施回溯并收集非采样工作流的痕迹，这些工作流会立即出现异常（例如，开始执行后不久就失败或返回错误代码）。然而，对于性能异常（例如，需要很长的时间来执行的请求），它是不够的。

> Unitary sampling: With this method, developers set the trace-point-sampling percentage directly and the sampling decision is made at the level of individual trace points. No attempt is made at coherence (i.e., capturing all trace points associated with a given workflow), so traces cannot be constructed using this approach. This method is best for use cases, such as resource attribution, where the information needed for analysis can be propagated with workflows (assuming dynamic, variable-width metadata) and retrieved at individual trace points directly.
**单一抽样**。在这种方法中，开发者直接设定跟踪点的采样比例，并在单个跟踪点的层面上做出采样决定。没有尝试一致性（即捕获与特定工作流相关的所有跟踪点），所以不能用这种方法构建跟踪。这种方法最适合用于资源归属等用例，在这些用例中，分析所需的信息可以通过工作流传播（假设是动态的、可变宽度的元数据），并直接在单个跟踪点上检索。
> In addition to deciding how to sample trace points, developers must decide how many of them to sample. Many infrastructures choose to randomly sample a small, set percentage—often between 0.01% and 10%—of trace points or workflows [9, 19, 40, 43](). However, this approach will capture only a few trace points for small workloads, limiting its use for them. Using per-workload sampling percentages can help, but this requires knowing workload sizes a priori. A more robust solution, proposed by Sigelman at al. [43](), is an adaptive scheme, in which the tracing infrastructure aims to always capture a set rate of trace points or workflows (e.g., 500 trace points/second or 100 workflows/second) and dynamically adjusts the trace-point- or workflow- sampling percentage to accomplish this set goal. Though promising, care must be taken to avoid biased results when the captured data is used for statistical purposes. For distributed services built on top of shared services, the adaptive sampling rate should be based on the tracing overhead the lowest-tier shared service can support (e.g., Bigtable [10]()) and proportionately propagated backward to top-tier services.

除了决定如何对跟踪点进行采样，开发者还必须决定对多少个跟踪点进行采样。许多基础设施选择随机取样一个小的、设定的百分比--通常在0.01%和10%的跟踪点或工作流之间[9, 19, 40, 43]()。然而，这种方法对于小的工作负载来说，只能捕捉到几个跟踪点，限制了它对它们的使用。使用每个工作负载的抽样百分比可以提供帮助，但这需要事先了解工作负载的大小。Sigelman等人[43]()提出的一个更稳健的解决方案是一个自适应方案，在这个方案中，追踪基础设施的目标是始终捕获设定的追踪点或工作流（例如，500个追踪点/秒或100个工作流/秒），并动态调整追踪点或工作流的采样百分比以实现这一设定目标。虽然很有希望，但当捕获的数据被用于统计目的时，必须注意避免有偏见的结果。对于建立在共享服务之上的分布式服务，自适应采样率应该基于最低层共享服务所能支持的追踪开销（例如Bigtable[10]()），并按比例向后传播到顶级服务。



[image-1]:	https://tva1.sinaimg.cn/large/008i3skNly1gsvn8fjyywj319i0cwwgq.jpg