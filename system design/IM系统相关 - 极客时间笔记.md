# 如何解决消息的实时到达问题
## 方法1 长轮询
长轮询模式是指当本次请求没有得到新消息时，并不会马上返回，而是会在服务端“悬挂”，等待一段时间；如果在等待时间内有新的消息产生，就能马上响应返回。
适用场景：对实时性要求比较高，但是整体用户量不太大。
缺点：服务端悬挂住请求，只是降低了无效请求的数量，并没有完全避免无效的请求。
服务端hang住，降低了入口qps，并没有减少对后端资源轮询的压力。
## 方法2: websocket（html5支持）
- 支持服务端推送的双向通信，大幅降低服务端轮询压力
- 数据交互的控制开销低，降低双方通信的网络开销
- Web 原生支持，实现相对简单
## 方法3: TCP 长连接
XMPP 协议、MQTT 协议以及各种私有协议衍生的 IM 协议
### XMPP 协议
#### 优点
- 成熟
- 扩展性不错
#### 缺点：
- 基于 XML 格式的协议传输上冗余比较多，在流量方面不太友好
- 整体实现上比较复杂，在如今移动网络场景下用得并不多。
### MQTT
#### 优点
- 基于代理的“发布 / 订阅”模式
- 省流量 扩展性方面都比较突出
#### 缺点
- 需要大量复杂的扩展和开发，比如不支持群组功能、不支持离线消息
# 如何保证消息的可靠投递
以“服务端路由中转”为例。
这和“消息队列如何保证消息的可靠投递” 或者 “主从间如何保证数据一致”采取的方案基本一致。
模型如下：
￼![][image-1]
这里保证可靠投递可以分成两部分：
1. 用户A — IM系统之间保证消息可靠投递
2. IM 系统 — 用户B之间如何保证消息可靠投递
## 用户A — IM系统之间可靠投递
客户端 A 的超时重发和 IM 服务器的去重机制
## IM 系统和用户B之间如何保证消息的可靠投递
业务层的 ACK 协议
IM 系统发送给用户B的消息中携带序列号SID，如果B成功收到消息，ACK中带着SID，确认B已经收到该消息，IM系统通过SID确认B已经收到消息，将SID从待确认列表列表中删除。
### prob1：如果消息在从IM系统发送到用户B的过程中丢失了，怎么处理？
- B 网络实际已经不可达，但 IM 服务器还没有感知到；
- 用户 B 的设备还没从内核缓冲区取完数据就崩溃了；
- 消息在中间网络途中被某些中间设备丢掉了，TCP 层还一直重传不成功等。
解决方案：
超时定时器。
等待响应的SID列表中维护一个超时定时器，如果超时，就重新发送。
### prob2: 采用超时定时器会存在，消息重复发送的问题，怎么处理？
解决方案：服务端推送消息时携带一个 Sequence ID，Sequence ID 在本次连接会话中需要唯一，针对同一条重推的消息 Sequence ID 不变，接收方根据这个唯一的 Sequence ID 来进行业务层的去重，这样经过去重后，对于用户 B 来说，看到的还是接收到一条消息，不影响使用体验。
### prob3: 如果IM系统在发送数据后宕机了，由于网络等其他原因导致B也没有收到消息，怎么处理？
这个就是类似“主从同步的原理”了，假设每个消息都存在一个单调递增的序号，B和IM系统重新建立连接时，把自己这边存储的最大的序号发送IM系统，IM系统就知道有哪些消息还没有发送给B（大于B提供的序号的消息都没有成功投递）。
B拉取新的消息。
### prob4: TCP协议本身提供消息可靠投递机制，为什么还需要ACK机制？
数据成功发送到接收方设备了，tcp层再把数据交给应用层时也可能出现异常情况。
![][image-2]
# 如何保证消息的时序一致性？
对于聊天、直播互动等业务来说，消息的时序代表的是发送方的意见表述和接收方的语义逻辑理解，如果时序一致性不能保证，可能就会造成聊天语义不连贯、容易出现曲解和误会。
## 问什么保证消息的时序一致性是一件比较困难的事？
原因：多发送方、多接收方、服务端多线程并发处理
## 解决方案
关键在于找到时序基准。使得我们的消息具备“时序可比较性”。
### 方案1: 使用本地时钟 或 本地序号
存在的问题： 时钟不同步导致消息顺序错乱。
### 方案2: 基于IM服务器的时钟
> 发送方把消息提交给 IM 服务器后，IM 服务器依据自身服务器的时钟生成一个时间戳，再把消息推送给接收方时携带这个时间戳，接收方依据这个时间戳来进行消息的排序。
> 在实际工程中，IM 服务都是集群化部署，集群化部署也就是许多服务器同时部署任务。虽然多台服务器通过 NTP 时间同步服务，能降低服务集群机器间的时钟差异到毫秒级别，但仍然还是存在一定的时钟误差，而且 IM 服务器规模相对比较大，时钟的统一性维护上也比较有挑战，整体时钟很难保持极低误差，因此一般也不能用 IM 服务器的本地时钟来作为消息的“时序基准”。
### 方案3: 全局序号生成器（全局唯一ID）
方案有很多，常见的比如 Redis 的原子自增命令 incr，DB 自带的自增 id，或者类似 Twitter 的 snowflake 算法、“时间相关”的分布式序号生成服务等。

# 消息的安全性如何保证
eg:
- DNS 劫持，导致发往 IM 服务的请求被拦截发到其他服务器，导致内容泄露或失效
- 明文传输的消息内容被中间设备劫取后篡改内容，再发往 IM 服务器引起业务错误等问题。
访问入口安全 - DNS劫持问题
传输链路安全 - 中间人攻击
# 分片上传：如何让你的图片、音视频消息发送得更快
## 多上传接入点
核心：避免跨运营商
传统的优化方式是：针对不同的主流运营商提供不同的上传接入点 IP，然后通过运营商 DNS 解析，让用户能通过本运营商的上传接入点来快速上传图片和视频；
图片上传存储服务，多线机房部署，这样上传服务也能快速地把文件流提交给存储层，从而避免从接入点到存储服务的跨网开销，也能解决其他运营商的用户下载图片时需要跨网的问题。
## 上传链路优化
区分多媒体消息上传通道和消息上传通道，发送多媒体消息时，先通过独立通道上传文件流，上传完成后会返回文件的唯一标识 ID，然后再把这个唯一标识 ID 作为消息的引用，通过普通消息收发通道进行发送。
![][image-3]
## 语音分片传输
语音存在录制时长，大小可控，考虑实时性、流畅，通过普通消息收发的长连通道来分片上传语音流。
异步存储，实时分片推到接收端，接收端接收之后不显示，直到接收到最后一条消息，接收端整合显示。

## [分片上传][1]
![][image-4]
(来源于阿里云oss官方文档)
在客户端把要上传的文件按照一定规则，分成多个数据块并标记序号，然后再分别上传，服务端接收到后，按照序号重新将多个数据块组装成文件
采用分片上传可以让客户端在分片完成后，利用“并行”的方式来同时上传多个分片，从而提升上传效率

在一些网络环境较差的场景下，采用“分片”的方式，可以在上传失败后进行重试时，不必重新上传整个文件，而只需要重传某一个失败的分片，这样也能提升重新发送的成功率和性能；
类似语音这种流式消息，在上传时并不知道文件最终大小，采用分片上传可以让消息文件先部分上传到服务器，而没必要等到录制完才开始上传，这样也能节约上传的整体时长。
### 分片分多大？
分片太大 - 片数少，上传的并发度不够，可能会降低上传效率，每个大的分片在失败后重传的成本会比较高
分片太小 - 片数多，并发需要的 TCP 连接太多，多条 TCP 连接的“窗口慢启动”会降低整体吞吐，两端拆分与合并分片的开销也相应增加，而且传输时的额外流量（HTTP 报头）也会更多。
- WiFi 下 2M
- 4G 下 1M
- 3G/2G 下 256K
很多大厂会尝试通过算法来“自适应动态根据网络现状调整分片大小”
“鱼翅”项目
### 断点续传
> 在使用分片上传的过程中，如果因意外导致上传失败，可以在重启的时候通过ListMultipartUploads和ListParts两个接口来获取某个Object上的所有的分片上传任务和每个分片上传任务中上传成功的Part列表。然后从最后一块成功上传的Part开始继续上传，从而达到断点续传的效果。

> 分片时会生成一个UploadID
> 断点续传时通过这个UploadID检查这个分片上传是否存在（出于成本考虑，只会暂存一段时间），获取upload信息，继续上传。
```python
Get  /ObjectName?uploadId=UploadId HTTP/1.1

Host: BucketName.oss.aliyuncs.com

Date: GMT Date

Authorization: Signature
```

```python
# 初始化
POST /ObjectName?uploads HTTP/1.1

Host: BucketName.oss.aliyuncs.com

Date: GMT date

Authorization: SignatureValue
```

```python
# 删除
DELETE /ObjectName?uploadId=UploadId HTTP/1.1
```

### 秒传机制
网络上流传度比较广的视频、图片，采用单向hash -\> 验证去重。
![][image-5]
减少用户流量，省电
hash碰撞存在误差，可以采用bloom filter，减少概率。

# 保证消息通道的高可用
## 解决“通道连不上”
- 暴露多个业界验证过比较安全的连接端口，来解决端口连通性问题
- 通过 HTTP Tunnel，来解决某些网络情况下只允许 HTTP 协议的数据传输的问题
- 通过 HttpDNS 和客户端预埋的方式，提供多个可选的通道接入点，让某些接入点在连不上时还能尝试其他接入点
## 解决“通道连接慢”
- 多运营商机房接入点，来避免用户的跨运营商网络访问；
- 对于提供的多接入点，客户端还可以通过“跑马竞速”的方式优先使用连接速度更快的接入点来访问
## 解决“通道不稳定”
服务端的架构设计着手，让我们的通道层服务和变化频繁的业务进行解耦，避免业务频繁变动导致通道层服务不稳定
对于消息下行通道压力大的业务场景，还可以隔离消息上下行通道，避免消息的上行被压力大的下行通道所影响
将多媒体的上传下载通道和消息收发的核心通道进行隔离，避免传输量大的多媒体消息造成通道的阻塞，影响消息收发。

# CDN 加速
CDN 加速技术，就是将客户端上传的图片、音视频发布到多个分布在**各地的 CDN 节点的服务器上**，当有用户需要访问这些图片和音视频时，能够通过** DNS 负载均衡技术**，根据用户来源就近访问 **CDN 节点中缓存的图片和音视频消息**，如果 CDN 节点中没有需要的资源，会先从源站同步到当前节点上，再返回给用户
** 源冗余的方式，既能显著提高用户访问的响应速度，也能有效缓解服务器访问量过大带来的对源存储服务的读写压力和带宽压力**
# 群聊消息的存储
对于群聊消息来说，是不是也需要给群里的每一个用户，都存储一条消息索引呢？
存一份还是存多份？
如果存多份，会存在写扩散，写的压力陡增，同时浪费存储空间。
如果群的规模比较小，例如微信，可以采用每个成员存一份。

存一份：读扩散，记录偏移量，群里用户需要查询消息时，都通过这个群维度的消息索引来获取。

```python
群表
group_id, user_id, created_time

消息表
msg_id, content,created_time
群聊消息表
group_id, msg_id, created_time
偏移量
user_id, last_msg
```


##  怎么保证新加入群的用户只看到新消息？
加入时间记为`added_time`
能看到的消息的创建时间必须 `>= added_time`
## 单个用户删除消息怎么办？
维护一张删除表，在其它设备上同步时，聚合两张表
# 监控
## 基于数据收集的被动监控
系统层监控、应用层监控及全链路 Trace 监控。
### 系统层监控
Nagios、Zabbix 等类似的系统监控工具，来实时收集机器相关的性能数据，如 CPU、内存、IO、负载、带宽、PPS、Swap 等数据信息
### 应用层级的监控




[1]:	https://help.aliyun.com/document_detail/31850.html

[image-1]:	https://tva1.sinaimg.cn/large/008i3skNly1gsd0s4gwozj30uu0bejs5.jpg
[image-2]:	https://tva1.sinaimg.cn/large/008i3skNly1gsd1nhlyj9j30o60dcjsb.jpg
[image-3]:	https://tva1.sinaimg.cn/large/008i3skNly1gsgglv3z9uj30sg0lcdh7.jpg
[image-4]:	https://tva1.sinaimg.cn/large/008i3skNly1gsgi8jy3fuj30w00mf3zx.jpg
[image-5]:	https://tva1.sinaimg.cn/large/008i3skNly1gsghwvwgtfj30vq0og41k.jpg