# 如何解决消息的实时到达问题
## 方法1 长轮询
长轮询模式是指当本次请求没有得到新消息时，并不会马上返回，而是会在服务端“悬挂”，等待一段时间；如果在等待时间内有新的消息产生，就能马上响应返回。
适用场景：对实时性要求比较高，但是整体用户量不太大。
缺点：服务端悬挂住请求，只是降低了无效请求的数量，并没有完全避免无效的请求。
服务端hang住，降低了入口qps，并没有减少对后端资源轮询的压力。
## 方法2: websocket（html5支持）
- 支持服务端推送的双向通信，大幅降低服务端轮询压力
- 数据交互的控制开销低，降低双方通信的网络开销
- Web 原生支持，实现相对简单
## 方法3: TCP 长连接
XMPP 协议、MQTT 协议以及各种私有协议衍生的 IM 协议
### XMPP 协议
#### 优点
- 成熟
- 扩展性不错
#### 缺点：
- 基于 XML 格式的协议传输上冗余比较多，在流量方面不太友好
- 整体实现上比较复杂，在如今移动网络场景下用得并不多。
### MQTT
#### 优点
- 基于代理的“发布 / 订阅”模式
- 省流量 扩展性方面都比较突出
#### 缺点
- 需要大量复杂的扩展和开发，比如不支持群组功能、不支持离线消息
# 如何保证消息的可靠投递
以“服务端路由中转”为例。
这和“消息队列如何保证消息的可靠投递” 或者 “主从间如何保证数据一致”采取的方案基本一致。
模型如下：
￼![][image-1]
这里保证可靠投递可以分成两部分：
1. 用户A — IM系统之间保证消息可靠投递
2. IM 系统 — 用户B之间如何保证消息可靠投递
## 用户A — IM系统之间可靠投递
客户端 A 的超时重发和 IM 服务器的去重机制
## IM 系统和用户B之间如何保证消息的可靠投递
业务层的 ACK 协议
IM 系统发送给用户B的消息中携带序列号SID，如果B成功收到消息，ACK中带着SID，确认B已经收到该消息，IM系统通过SID确认B已经收到消息，将SID从待确认列表列表中删除。
### prob1：如果消息在从IM系统发送到用户B的过程中丢失了，怎么处理？
- B 网络实际已经不可达，但 IM 服务器还没有感知到；
- 用户 B 的设备还没从内核缓冲区取完数据就崩溃了；
- 消息在中间网络途中被某些中间设备丢掉了，TCP 层还一直重传不成功等。
解决方案：
超时定时器。
等待响应的SID列表中维护一个超时定时器，如果超时，就重新发送。
### prob2: 采用超时定时器会存在，消息重复发送的问题，怎么处理？
解决方案：服务端推送消息时携带一个 Sequence ID，Sequence ID 在本次连接会话中需要唯一，针对同一条重推的消息 Sequence ID 不变，接收方根据这个唯一的 Sequence ID 来进行业务层的去重，这样经过去重后，对于用户 B 来说，看到的还是接收到一条消息，不影响使用体验。
### prob3: 如果IM系统在发送数据后宕机了，由于网络等其他原因导致B也没有收到消息，怎么处理？
这个就是类似“主从同步的原理”了，假设每个消息都存在一个单调递增的序号，B和IM系统重新建立连接时，把自己这边存储的最大的序号发送IM系统，IM系统就知道有哪些消息还没有发送给B（大于B提供的序号的消息都没有成功投递）。
B拉取新的消息。
### prob4: TCP协议本身提供消息可靠投递机制，为什么还需要ACK机制？
数据成功发送到接收方设备了，tcp层再把数据交给应用层时也可能出现异常情况。
![][image-2]
# 如何保证消息的时序一致性？
对于聊天、直播互动等业务来说，消息的时序代表的是发送方的意见表述和接收方的语义逻辑理解，如果时序一致性不能保证，可能就会造成聊天语义不连贯、容易出现曲解和误会。
## 问什么保证消息的时序一致性是一件比较困难的事？
原因：多发送方、多接收方、服务端多线程并发处理
## 解决方案
关键在于找到时序基准。使得我们的消息具备“时序可比较性”。
### 方案1: 使用本地时钟 或 本地序号
存在的问题： 时钟不同步导致消息顺序错乱。
### 方案2: 基于IM服务器的时钟
> 发送方把消息提交给 IM 服务器后，IM 服务器依据自身服务器的时钟生成一个时间戳，再把消息推送给接收方时携带这个时间戳，接收方依据这个时间戳来进行消息的排序。
> 在实际工程中，IM 服务都是集群化部署，集群化部署也就是许多服务器同时部署任务。虽然多台服务器通过 NTP 时间同步服务，能降低服务集群机器间的时钟差异到毫秒级别，但仍然还是存在一定的时钟误差，而且 IM 服务器规模相对比较大，时钟的统一性维护上也比较有挑战，整体时钟很难保持极低误差，因此一般也不能用 IM 服务器的本地时钟来作为消息的“时序基准”。
### 方案3: 全局序号生成器（全局唯一ID）
方案有很多，常见的比如 Redis 的原子自增命令 incr，DB 自带的自增 id，或者类似 Twitter 的 snowflake 算法、“时间相关”的分布式序号生成服务等。

# 消息的安全性如何保证
eg:
- DNS 劫持，导致发往 IM 服务的请求被拦截发到其他服务器，导致内容泄露或失效
- 明文传输的消息内容被中间设备劫取后篡改内容，再发往 IM 服务器引起业务错误等问题。
访问入口安全 - DNS劫持问题
传输链路安全 - 中间人攻击
# 分片上传：如何让你的图片、音视频消息发送得更快
## 多上传接入点
核心：避免跨运营商
传统的优化方式是：针对不同的主流运营商提供不同的上传接入点 IP，然后通过运营商 DNS 解析，让用户能通过本运营商的上传接入点来快速上传图片和视频；
图片上传存储服务，多线机房部署，这样上传服务也能快速地把文件流提交给存储层，从而避免从接入点到存储服务的跨网开销，也能解决其他运营商的用户下载图片时需要跨网的问题。
## 上传链路优化
区分多媒体消息上传通道和消息上传通道，发送多媒体消息时，先通过独立通道上传文件流，上传完成后会返回文件的唯一标识 ID，然后再把这个唯一标识 ID 作为消息的引用，通过普通消息收发通道进行发送。
![][image-3]
## 语音分片传输
语音存在录制时长，大小可控，考虑实时性、流畅，通过普通消息收发的长连通道来分片上传语音流。
异步存储，实时分片推到接收端，接收端接收之后不显示，直到接收到最后一条消息，接收端整合显示。

## 分片上传
在客户端把要上传的文件按照一定规则，分成多个数据块并标记序号，然后再分别上传，服务端接收到后，按照序号重新将多个数据块组装成文件
采用分片上传可以让客户端在分片完成后，利用“并行”的方式来同时上传多个分片，从而提升上传效率

在一些网络环境较差的场景下，采用“分片”的方式，可以在上传失败后进行重试时，不必重新上传整个文件，而只需要重传某一个失败的分片，这样也能提升重新发送的成功率和性能；
类似语音这种流式消息，在上传时并不知道文件最终大小，采用分片上传可以让消息文件先部分上传到服务器，而没必要等到录制完才开始上传，这样也能节约上传的整体时长。
### 分片分多大？
分片太大 - 片数少，上传的并发度不够，可能会降低上传效率，每个大的分片在失败后重传的成本会比较高
分片太小 - 片数多，并发需要的 TCP 连接太多，多条 TCP 连接的“窗口慢启动”会降低整体吞吐，两端拆分与合并分片的开销也相应增加，而且传输时的额外流量（HTTP 报头）也会更多。
- WiFi 下 2M
- 4G 下 1M
- 3G/2G 下 256K
很多大厂会尝试通过算法来“自适应动态根据网络现状调整分片大小”
“鱼翅”项目
### 断点续传
分片时会生成一个UploadID
断点续传时通过这个UploadID检查这个分片上传是否存在（出于成本考虑，只会暂存一段时间），获取upload信息，继续上传。
```python
Get  /ObjectName?uploadId=UploadId HTTP/1.1

Host: BucketName.oss.aliyuncs.com

Date: GMT Date

Authorization: Signature
```

```python
# 初始化
POST /ObjectName?uploads HTTP/1.1

Host: BucketName.oss.aliyuncs.com

Date: GMT date

Authorization: SignatureValue
```

```python
# 删除
DELETE /ObjectName?uploadId=UploadId HTTP/1.1
```

### 秒传机制
网络上流传度比较广的视频、图片，采用单向hash -\> 验证去重。
![][image-4]
减少用户流量，省电
hash碰撞存在误差，可以采用bloom filter，减少概率。



[image-1]:	https://tva1.sinaimg.cn/large/008i3skNly1gsd0s4gwozj30uu0bejs5.jpg
[image-2]:	https://tva1.sinaimg.cn/large/008i3skNly1gsd1nhlyj9j30o60dcjsb.jpg
[image-3]:	https://tva1.sinaimg.cn/large/008i3skNly1gsgglv3z9uj30sg0lcdh7.jpg
[image-4]:	https://tva1.sinaimg.cn/large/008i3skNly1gsghwvwgtfj30vq0og41k.jpg